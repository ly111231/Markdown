**Batch Normalization**：对每列（特征）进行归一化，适合特征在不同批次样本中相对一致的情况。

**Layer Normalization**：对每行（样本）进行归一化，适合每个样本特征间关系不同、且独立于批次的情况

**DropOut / DropPath**：

| 特性         | DropOut                        | DropPath                                         |
| ------------ | ------------------------------ | ------------------------------------------------ |
| 主要作用对象 | 单个神经元                     | 整个子模块（路径）                               |
| 应用场景     | 通常用于全连接层，适合各种网络 | 适合深度网络的子模块（如 Transformer 和 ResNet） |
| 目的         | 随机抑制神经元，防止过拟合     | 随机丢弃路径的输出，提高网络的鲁棒性             |

两者都可以在模型中引入正则化效果，提升模型的泛化能力，但由于 `DropPath` 的丢弃方式更适合深层模块化网络，所以它在 Transformer 和 CNN 网络复现时经常代替 `DropOut`。